{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anomaly detection on Yahoo Benchmark\n",
    "\n",
    "#--------------------------\n",
    "# Step 1 Import Libraries\n",
    "#--------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pdf as pdf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import lognorm\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "#----------------------\n",
    "# Step 2 Load Dataset\n",
    "# ---------------------\n",
    "os.chdir('/Users/sylviachadha/Desktop/Anomaly_Detection/Practice_Dataset')\n",
    "df = pd.read_csv('real_51.csv', index_col = 'timestamp')\n",
    "print(df.head(6))\n",
    "\n",
    "#-------------------\n",
    "# Step3 Plot data\n",
    "#-------------------\n",
    "ax = df['value'].plot(figsize = (12,6), title = 'yahoo traffic data');\n",
    "xlabel = 'hourly data'\n",
    "ylabel = 'traffic - yahoo properties'\n",
    "ax.set(xlabel = xlabel, ylabel = ylabel)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Step4 Train, Validation, Test Split\n",
    "# ------------------------------------\n",
    "\n",
    "train_size = 0.70\n",
    "validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "\n",
    "train, remain = train_test_split(df, test_size=(validation_size + test_size),shuffle=False)\n",
    "\n",
    "new_test_size = np.around(test_size / (validation_size + test_size), 2)\n",
    "# To preserve (new_test_size + new_val_size) = 1.0\n",
    "new_val_size = 1.0 - new_test_size\n",
    "\n",
    "val, test = train_test_split(remain, test_size=new_test_size,shuffle=False)\n",
    "\n",
    "print(train)\n",
    "print(val)\n",
    "print(test)\n",
    "\n",
    "len(df)\n",
    "\n",
    "# Store Actual labels for val data to use for threshold decision later\n",
    "\n",
    "no_of_time_steps=10\n",
    "print(val)\n",
    "len(val)\n",
    "val_actual = val.iloc[no_of_time_steps:]\n",
    "print(val_actual)\n",
    "\n",
    "# Store Actual labels for test data to use for confusion matrix evaluation\n",
    "\n",
    "no_of_time_steps=10\n",
    "print(test)\n",
    "len(test)\n",
    "test_actual = test.iloc[no_of_time_steps:]\n",
    "print(test_actual)\n",
    "\n",
    "# Check ratio of classes (Normal vs Anomalies)\n",
    "df['is_anomaly'].value_counts()\n",
    "\n",
    "# To check how many anomalies are present in Training and how many in validation and test\n",
    "a = train.loc[train.is_anomaly == 1]\n",
    "print(a)\n",
    "total_rowsa = a['is_anomaly'].count()\n",
    "print('is_anomaly_training_count',total_rowsa)\n",
    "\n",
    "\n",
    "b = val.loc[val.is_anomaly == 1]\n",
    "print(b)\n",
    "total_rowsb = b['is_anomaly'].count()\n",
    "print('is_anomaly_val_count',total_rowsb)\n",
    "\n",
    "c = test.loc[test.is_anomaly == 1]\n",
    "print(c)\n",
    "total_rowsc = c['is_anomaly'].count()\n",
    "print('is_anomaly_test_count',total_rowsc)\n",
    "\n",
    "# Anomalies still present in test set\n",
    "d = test.loc[test.is_anomaly == 1]\n",
    "total_rows = d['is_anomaly'].count()\n",
    "print('is_anomaly_test_count',total_rows)\n",
    "\n",
    "# Remove is_anomaly = 1 rows from only train data\n",
    "train = train.drop(train[train.is_anomaly == 1].index)\n",
    "print('new_train_count',train['value'].count())\n",
    "\n",
    "# Just to confirm no anomaly in training set\n",
    "c = train.loc[train.is_anomaly == 1]\n",
    "total_rows = c['is_anomaly'].count()\n",
    "print('is_anomaly_training_count',total_rows)\n",
    "\n",
    "# Remove is_anomaly column from train and test data since semi-supervised learning\n",
    "# For validation do not remove \"is_anomaly\" since need actual labels to decide on threshold\n",
    "del train['is_anomaly']\n",
    "del test['is_anomaly']\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "#-------------------------\n",
    "# Step 5 Scaling of data\n",
    "#-------------------------\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler = scaler.fit(train[['value']])\n",
    "\n",
    "train['value'] = scaler.transform(train[['value']])\n",
    "val['value'] = scaler.transform(val[['value']])\n",
    "test['value'] = scaler.transform(test[['value']])\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "# Step 6 - Prepare Input for LSTM\n",
    "#-----------------------------------\n",
    "# Weâ€™ll split the data into sub-sequences - changing input to a shape as accepted by\n",
    "# lstm autoencoder\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS=10\n",
    "\n",
    "# Reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(\n",
    "  train[['value']],\n",
    "  train.value,\n",
    "  TIME_STEPS\n",
    ")\n",
    "\n",
    "X_val, y_val = create_dataset(\n",
    "  val[['value']],\n",
    "  val.value,\n",
    "  TIME_STEPS\n",
    ")\n",
    "\n",
    "X_test, y_test = create_dataset(\n",
    "  test[['value']],\n",
    "  test.value,\n",
    "  TIME_STEPS\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#-----------------------------------\n",
    "# Step 7 Define Model Architecture\n",
    "#-----------------------------------\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(\n",
    "    units=32,\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "model.add(keras.layers.LSTM(units=32, return_sequences=True))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(\n",
    "  keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(units=X_train.shape[2])\n",
    "  )\n",
    ")\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#----------------------------\n",
    "# Step 8 Training/Fit Model\n",
    "#----------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=10,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Step 9 Predict & decide parameters using MLE on Training data\n",
    "#------------------------------------------------------------\n",
    "X_train_pred = model.predict(X_train)\n",
    "\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
    "\n",
    "# Show distribution plot of training loss\n",
    "sns.distplot(train_mae_loss, bins=50, kde=True);\n",
    "plt.show()\n",
    "\n",
    "train_mae_loss = train_mae_loss.flatten()\n",
    "\n",
    "# We normally assume distributions on errors/residuals\n",
    "# Assume guassian distribution for prediction errors on training data\n",
    "train_mae_loss\n",
    "\n",
    "# Use MLE to estimate best parameters for above prediction errors\n",
    "parameters = norm.fit(train_mae_loss)\n",
    "print (\"Mean and S.D obtained using MLE on training prediction errors is\",parameters)\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Step 10 - Method to decide on Threshold value\n",
    "#-----------------------------------------------\n",
    "\n",
    "# Apply trained model on validation set and get validation prediction errors\n",
    "# Adding anomalies of Training as well to validation\n",
    "\n",
    "\n",
    "X_val_pred = model.predict(X_val)\n",
    "\n",
    "val_mae_loss = np.mean(np.abs(X_val_pred - X_val), axis=1)\n",
    "len(val_mae_loss)\n",
    "\n",
    "# Compute probability values for errors on validation set\n",
    "\n",
    "prob_val_mae_loss = norm.pdf(val_mae_loss, loc=parameters[0], scale=parameters[1])\n",
    "prob_val_mae_loss = prob_val_mae_loss.flatten()\n",
    "prob_val_mae_loss_log = np.log(prob_val_mae_loss)\n",
    "prob_val_mae_loss_log\n",
    "\n",
    "# A threshold is set on the log probability distribution values\n",
    "\n",
    "#----------------#\n",
    "# Assumptions\n",
    "#----------------#\n",
    "# 1. An anomaly is an observation which is suspected of being partially or wholly irrelevant\n",
    "# because it is not generated by the stochastic model assumed.Above we assumed guassian\n",
    "# distribution on training errors train_mae_loss\n",
    "\n",
    "# 2. Normal data instances occur in high probability regions of a stochastic model,\n",
    "# while anomalies occur in the low probability regions of the stochastic model.\n",
    "# This assumption is checked below where low Probability density instances should be\n",
    "# captured as anomalies\n",
    "\n",
    "error_df = pd.DataFrame({'probability_val_set': prob_val_mae_loss_log,\n",
    "                        'True_class_val_set': val_actual['is_anomaly']})\n",
    "\n",
    "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class_val_set,error_df.probability_val_set)\n",
    "\n",
    "plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\n",
    "\n",
    "plt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\n",
    "\n",
    "plt.title('Precision and recall for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# See values of different thresholds as per precision_recall_curve function\n",
    "print(threshold_rt)\n",
    "print(precision_rt)\n",
    "print(recall_rt)\n",
    "\n",
    "# Choose threshold which maximize the F1 Score\n",
    "\n",
    "f1_scores = 2*recall_rt*precision_rt/(recall_rt+precision_rt)\n",
    "\n",
    "selected_threshold = threshold_rt[np.argmax(f1_scores)]\n",
    "print('Best threshold: ', threshold_rt[np.argmax(f1_scores)])\n",
    "print('Best F1-Score: ', np.max(f1_scores))\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Step 13 - Get predictions on test set\n",
    "#-----------------------------------------------\n",
    "\n",
    "X_test_pred = model.predict(X_test)\n",
    "\n",
    "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "len(test_mae_loss)\n",
    "\n",
    "# Compute probability values for errors on test set\n",
    "\n",
    "prob_test_mae_loss = norm.pdf(test_mae_loss, loc=parameters[0], scale=parameters[1])\n",
    "prob_test_mae_loss = prob_test_mae_loss.flatten()\n",
    "prob_test_mae_loss_log = np.log(prob_test_mae_loss)\n",
    "prob_test_mae_loss_log\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Step 12 - Evaluate this threshold on test set\n",
    "#-----------------------------------------------\n",
    "\n",
    "test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "test_score_df['loss'] = prob_test_mae_loss_log\n",
    "test_score_df['threshold'] = selected_threshold\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['anomaly'] = test_score_df['anomaly'].astype(int)\n",
    "test_score_df['value'] = test[TIME_STEPS:].value\n",
    "\n",
    "#--------------------------------------------\n",
    "# Step 13 Plot Test Loss and Threshold\n",
    "#---------------------------------------------\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "plt.show()\n",
    "\n",
    "#---------------------------------------------\n",
    "# Step 14 Print Anomalies Actual and Predicted\n",
    "#---------------------------------------------\n",
    "\n",
    "anomalies = test_score_df[test_score_df.anomaly == 1]\n",
    "print ('ACTUAL ANOMALIES count',len(d))\n",
    "print('ACTUAL ANOMALIES',d)\n",
    "print ('DETECTED ANOMALIES count',len(anomalies))\n",
    "print('DETECTED ANOMALIES', anomalies)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Step 15 Evaluation metrics with test_actual and test_score_df\n",
    "# 1. Confusion Matrix\n",
    "# 2. Accuracy\n",
    "# 3. Precision\n",
    "# 4. Recall\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(test_actual['is_anomaly'],test_score_df['anomaly'])\n",
    "print('Confusion Matrix')\n",
    "print(cf)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(test_actual['is_anomaly'],test_score_df['anomaly'])\n",
    "print('Accuracy')\n",
    "print(acc)\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(test_actual['is_anomaly'],test_score_df['anomaly'])\n",
    "print('Recall')\n",
    "print(recall)\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(test_actual['is_anomaly'],test_score_df['anomaly'])\n",
    "print('Precision')\n",
    "print(precision)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}